{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BaggingExample.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNeoAJ2Ac1j0MdSuFmy/hS9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gnitnaw/Programs/blob/master/Chapter8/BaggingExample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaWHJhVsAblT",
        "outputId": "43eaf95f-ecf4-442c-c28c-84cd392e19c7"
      },
      "source": [
        "!python --version"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LetQC4wyAgKh"
      },
      "source": [
        "# Example 8.3\n",
        "## Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bXKlDOvAoW5",
        "outputId": "677977a1-2261-4790-b58e-45839f223fec"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_friedman1\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split, cross_validate\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "print(np.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.21.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmrPRrN05iIF"
      },
      "source": [
        "## Set random seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdzVvsn_5lIf"
      },
      "source": [
        "np.random.seed(100)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYGyiNcLBI8j"
      },
      "source": [
        "## Original Program"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OGRmXk9BLh2",
        "outputId": "b2df3450-90a5-428d-c670-85a818da0944"
      },
      "source": [
        "# create regression problem\n",
        "n_points = 1000 # points\n",
        "x, y =  make_friedman1(n_samples=n_points, n_features=15, \n",
        "                       noise=1.0, random_state=100)\n",
        "\n",
        "# split to train/test set\n",
        "x_train, x_test, y_train, y_test = \\\n",
        "        train_test_split(x, y, test_size=0.33, random_state=100)\n",
        "\n",
        "# training\n",
        "regTree = DecisionTreeRegressor(random_state=100)\n",
        "regTree.fit(x_train,y_train)\n",
        "\n",
        "# test\n",
        "yhat = regTree.predict(x_test)\n",
        "\n",
        "# Bagging construction\n",
        "n_estimators=500\n",
        "bag = np.empty((n_estimators), dtype=object)\n",
        "bootstrap_ds_arr = np.empty((n_estimators), dtype=object)\n",
        "for i in range(n_estimators):\n",
        "    # sample bootsraped dataset\n",
        "    ids = np.random.choice(range(0,len(x_test)),size=len(x_test),\n",
        "                     replace=True)\n",
        "    x_boot = x_train[ids]\n",
        "    y_boot = y_train[ids]\n",
        "    bootstrap_ds_arr[i] = np.unique(ids)\n",
        "    \n",
        "    bag[i] = DecisionTreeRegressor()\n",
        "    bag[i].fit(x_boot,y_boot)\n",
        "\n",
        "# bagging prediction\n",
        "yhatbag = np.zeros(len(y_test))   \n",
        "for i in range(n_estimators): \n",
        "    yhatbag = yhatbag + bag[i].predict(x_test)\n",
        "        \n",
        "yhatbag = yhatbag/n_estimators\n",
        "\n",
        "# out of bag loss estimation\n",
        "oob_pred_arr = np.zeros(len(x_train))\n",
        "for i in range(len(x_train)):\n",
        "    x = x_train[i].reshape(1, -1)\n",
        "    C = []\n",
        "    for b in range(n_estimators):\n",
        "        if(np.isin(i, bootstrap_ds_arr[b])==False):\n",
        "            C.append(b)\n",
        "    for pred in bag[C]:       \n",
        "        oob_pred_arr[i] = oob_pred_arr[i] + (pred.predict(x)/len(C))        \n",
        "\n",
        "L_oob = r2_score(y_train, oob_pred_arr)\n",
        "\n",
        "print(\"DecisionTreeRegressor R^2 score = \",r2_score(y_test, yhat),  \n",
        "      \"\\nBagging R^2 score = \", r2_score(y_test, yhatbag),\n",
        "      \"\\nBagging OOB R^2 score = \",L_oob)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DecisionTreeRegressor R^2 score =  0.5652585102808715 \n",
            "Bagging R^2 score =  0.7594897731192692 \n",
            "Bagging OOB R^2 score =  0.7755235429272777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare loss of N-fold cross-validation and OOB"
      ],
      "metadata": {
        "id": "GArerz-wUV8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 1000\n",
        "Bmax = 500\n",
        "X, Y =  make_friedman1(n_samples=n, n_features=15, noise=1.0, random_state=100)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=100)\n",
        "clf = DecisionTreeRegressor(random_state=0)\n",
        "cv_results = cross_validate(clf, X_train, Y_train, cv=Y_train.shape[0], scoring ='neg_mean_squared_error', return_train_score=True)\n",
        "\n",
        "error_B = np.zeros(Bmax)\n",
        "OOB_B = np.zeros(Bmax)\n",
        "# Bagging construction\n",
        "n_estimators=Bmax\n",
        "bag = np.empty((n_estimators), dtype=object)\n",
        "bootstrap_ds_arr = np.empty((n_estimators), dtype=object)\n",
        "for i in range(n_estimators):\n",
        "    # sample bootsraped dataset\n",
        "    ids = np.random.choice(range(0,len(X_train)),size=len(X_train),\n",
        "                     replace=True)\n",
        "    \n",
        "    x_boot = X_train[ids]\n",
        "    y_boot = Y_train[ids]\n",
        "    bootstrap_ds_arr[i] = np.unique(ids)\n",
        "    bag[i] = DecisionTreeRegressor()\n",
        "    bag[i].fit(x_boot,y_boot)\n",
        "\n",
        "# bagging prediction\n",
        "yhatbag = np.zeros(len(Y_test))   \n",
        "for i in range(n_estimators): \n",
        "    yhatbag = yhatbag + bag[i].predict(X_test)\n",
        "    error_B[i] = mean_squared_error(Y_test, yhatbag/(i+1))\n",
        "\n",
        "# out of bag loss estimation\n",
        "oob_pred_arr = np.zeros(len(X_train))\n",
        "for i in range(len(X_train)):\n",
        "    x = X_train[i].reshape(1, -1)\n",
        "    C = []\n",
        "    for b in range(n_estimators):\n",
        "        if(np.isin(i, bootstrap_ds_arr[b])==False):\n",
        "            C.append(b)\n",
        "    #print(len(C))\n",
        "    for pred in  bag[C]:       \n",
        "        oob_pred_arr[i] = oob_pred_arr[i] + (pred.predict(x)/len(C))\n",
        "\n",
        "LL_oob = mean_squared_error(Y_train, oob_pred_arr)\n",
        "print(\"cross_validate loss = \", np.mean(cv_results['test_score']), \n",
        "      \"\\nBagging OOB MSE score = \",LL_oob)\n",
        "print(error_B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6lIdPKjUgw5",
        "outputId": "330e03e9-cd28-42f3-9433-ace46c938b3a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cross_validate loss =  -9.25439805855385 \n",
            "Bagging OOB MSE score =  4.339658951166955\n",
            "[12.57943888  9.69760299  8.37439952  7.23119866  6.89097184  6.70714173\n",
            "  6.41927994  6.27012489  6.18052977  6.09244196  6.05690628  5.98188085\n",
            "  5.85176005  5.6993827   5.69934466  5.63317978  5.58116661  5.4542918\n",
            "  5.44618434  5.50870398  5.42924013  5.44626065  5.52056038  5.42594414\n",
            "  5.38024633  5.34380359  5.27987388  5.28907612  5.34522285  5.33414652\n",
            "  5.31258058  5.28481526  5.23779702  5.23209615  5.22672713  5.21694649\n",
            "  5.20577009  5.19319425  5.17886283  5.19247903  5.16110621  5.18340333\n",
            "  5.17720803  5.17695094  5.15135618  5.1577897   5.12670755  5.11420038\n",
            "  5.09963584  5.11308145  5.10711712  5.11340031  5.08058492  5.08207168\n",
            "  5.08203463  5.09064725  5.06085549  5.0524102   5.05710643  5.04816921\n",
            "  5.04524883  5.05552588  5.04555752  5.06363624  5.07610039  5.07787476\n",
            "  5.10143426  5.09460889  5.10719633  5.09745801  5.0784167   5.07588832\n",
            "  5.06568152  5.06418016  5.04815061  5.05912473  5.07724903  5.07002267\n",
            "  5.07520117  5.06731844  5.0689726   5.07439425  5.06631276  5.06181478\n",
            "  5.06450286  5.06878769  5.07316856  5.07193637  5.06031631  5.06864974\n",
            "  5.07159816  5.06453425  5.06343138  5.06771805  5.06287411  5.05220181\n",
            "  5.05653524  5.05646524  5.05611862  5.06938413  5.05690713  5.04452447\n",
            "  5.04888196  5.04924709  5.03401091  5.03431874  5.0298362   5.01980808\n",
            "  5.01500814  5.01622019  5.01843994  5.0120594   5.00680707  5.01184759\n",
            "  5.01370093  5.01662764  5.01132308  5.01251654  5.01729653  5.01965549\n",
            "  5.01145461  5.01588526  5.00369912  5.00614695  5.01012821  5.0091022\n",
            "  5.01357416  5.00232363  4.98460903  4.97776814  4.98177408  4.98261712\n",
            "  4.97893909  4.96726852  4.97671102  4.98179619  4.97159949  4.97776053\n",
            "  4.98252403  4.97709408  4.97295063  4.96871924  4.97009638  4.97921907\n",
            "  4.97863292  4.9733003   4.97369459  4.96429603  4.95796705  4.95298092\n",
            "  4.9524093   4.96182843  4.95023872  4.94245582  4.94328182  4.94356727\n",
            "  4.95018266  4.95273665  4.95479732  4.96011379  4.96320334  4.96197404\n",
            "  4.96129019  4.95564952  4.95966179  4.96133539  4.96244814  4.9642854\n",
            "  4.96253915  4.96520042  4.96337694  4.96816996  4.96346477  4.96623387\n",
            "  4.96054305  4.96692068  4.96341144  4.96661048  4.96427893  4.95978186\n",
            "  4.95708234  4.95485295  4.96461501  4.96879892  4.96874855  4.97019338\n",
            "  4.9753495   4.96880985  4.96740986  4.97103557  4.97184669  4.97512361\n",
            "  4.96901705  4.97282272  4.96982658  4.97230479  4.97254464  4.96926089\n",
            "  4.9721276   4.96892043  4.97013086  4.97237665  4.97824033  4.98331278\n",
            "  4.98230217  4.97487239  4.97245426  4.97752529  4.98072142  4.98405931\n",
            "  4.98368233  4.98673919  4.98700551  4.9863387   4.97685241  4.97953107\n",
            "  4.98306166  4.98341803  4.98091389  4.98345342  4.9836081   4.97766151\n",
            "  4.97013394  4.97040638  4.97227121  4.97076745  4.96471946  4.96861407\n",
            "  4.96760392  4.97144613  4.97272898  4.97644536  4.97614326  4.97258161\n",
            "  4.97202827  4.97615992  4.97586923  4.97610319  4.9741033   4.97693137\n",
            "  4.98164082  4.98691139  4.99343944  4.99646398  5.00015215  4.99935448\n",
            "  5.00060831  4.9937351   4.99374158  4.99577976  4.99745883  4.9974414\n",
            "  4.99531682  4.9924588   4.99464705  4.99091608  4.99352718  4.99332588\n",
            "  4.99617364  4.99803536  4.99576194  4.9982311   4.99976475  4.9994744\n",
            "  4.99646592  5.00008167  4.99958327  5.00056918  4.99565656  4.99231701\n",
            "  4.99123734  4.98874491  4.98524086  4.98607596  4.98577158  4.98194342\n",
            "  4.97965097  4.97748609  4.98297091  4.98515391  4.98115381  4.98111346\n",
            "  4.98252022  4.98445697  4.98354864  4.98043728  4.98062242  4.97715344\n",
            "  4.97801961  4.97615355  4.97404454  4.97529153  4.97451844  4.9730873\n",
            "  4.97359423  4.97219793  4.97218693  4.97073753  4.97352714  4.97364593\n",
            "  4.97162957  4.97124816  4.96738881  4.96445321  4.96809621  4.97151609\n",
            "  4.97199233  4.97255093  4.97561389  4.97819076  4.9794207   4.9802358\n",
            "  4.98025195  4.98349497  4.98163516  4.98382816  4.98235377  4.98243871\n",
            "  4.98171658  4.98331656  4.98622731  4.98600605  4.98444853  4.97859027\n",
            "  4.97991962  4.98260989  4.98021284  4.97803117  4.98013329  4.97854908\n",
            "  4.98358942  4.9858852   4.98434281  4.98681461  4.98595818  4.98756205\n",
            "  4.9903728   4.99252692  4.99318997  4.99617337  4.99792258  4.99709816\n",
            "  4.99657428  4.99223595  4.9977544   4.99587474  4.99604082  4.99143977\n",
            "  4.99223276  4.99534503  4.99247375  4.99128145  4.99260065  4.99399159\n",
            "  4.99614131  4.99691732  4.99729849  4.99774679  4.99558736  4.9960697\n",
            "  4.99543415  4.99303511  4.99383158  4.99269881  4.99098965  4.99001115\n",
            "  4.99046041  4.98960994  4.99093125  4.99114524  4.98699059  4.98718202\n",
            "  4.98621583  4.98179467  4.98248276  4.97934403  4.97717794  4.9809317\n",
            "  4.98089187  4.98251973  4.98423737  4.98474572  4.98470126  4.98298564\n",
            "  4.98451998  4.98771077  4.9870597   4.98320858  4.98351066  4.98294676\n",
            "  4.98052806  4.98083661  4.97808015  4.9794202   4.97977285  4.97953542\n",
            "  4.98177924  4.98491496  4.98131858  4.97912898  4.97831559  4.98059623\n",
            "  4.98144729  4.98111757  4.97547134  4.97328667  4.97354733  4.97387814\n",
            "  4.97483325  4.97773307  4.97635405  4.97691954  4.97800128  4.97593976\n",
            "  4.97850252  4.98074943  4.97953078  4.98188929  4.98052532  4.98222475\n",
            "  4.98431259  4.98433438  4.98351819  4.98411845  4.98194067  4.98343416\n",
            "  4.98430665  4.9857017   4.98840359  4.98987945  4.98617383  4.98508836\n",
            "  4.98459122  4.98533955  4.98715931  4.98678601  4.98940523  4.98922897\n",
            "  4.98936286  4.98903175  4.98813531  4.98266471  4.97919953  4.97508887\n",
            "  4.97551261  4.97601423  4.97503218  4.97066465  4.97095288  4.97206622\n",
            "  4.97578679  4.97374622  4.97343926  4.97077043  4.97114031  4.96873575\n",
            "  4.96775144  4.96916318  4.97062295  4.96913561  4.97080982  4.97134448\n",
            "  4.97198669  4.97233426  4.97110466  4.97196895  4.97222121  4.97237029\n",
            "  4.97094741  4.96858335  4.96660191  4.96714616  4.96805685  4.96805596\n",
            "  4.97094738  4.97151809  4.96914787  4.96996018  4.97068163  4.97319288\n",
            "  4.9716142   4.9691024   4.97105043  4.97159452  4.9724123   4.96912733\n",
            "  4.96897084  4.9700823   4.96949086  4.96875273  4.96897241  4.96825785\n",
            "  4.96674114  4.96455695  4.96690412  4.96648288  4.96684357  4.96609323\n",
            "  4.96586208  4.96764616]\n"
          ]
        }
      ]
    }
  ]
}